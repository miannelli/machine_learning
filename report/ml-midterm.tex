\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

\title{The Ramifications of High Dimensional Spaces on Machine Learning Techniques}
\author{Jonathan Gryak
\qquad
Michael Iannelli\\
PhD Program in Computer Science\\
CUNY Graduate Center\\
City University of New York\\
\texttt{\{jgryak, miannelli\}@gradcenter.cuny.edu}}
\date{}
\begin{document}

\maketitle

\begin{abstract}
Stuff
\end{abstract}
\tableofcontents
\section{Introduction}
motivation:\\
-nearest neighbor\\
-k-means\\
-similarity indexing\\
need more references for the use of these in high dimensions\\
in the introduction to machine learning book, it speaks of dimensionality with respect to density estimation,
not sure if that's useful here

\

The ``curse of dimensionality" coined by Richard Bellman in 1961\cite{bellman1961adaptive}, comes in many forms, including Bellman's original context of optimization, functional approximation, and combinatorics. In this era of ``big data", ``big" not only refers to the indefatigable increase the amount of data collected, but also in the number of features and components that are utilized in the data analysis. However,  as shown in\cite{aggarwal2001surprising}, the increase in the number of features considered, which are usually considered as a multi-dimensional space, has grave repercussions for machine learning techniques that rely on some measure of distance between points in the feature space. Moreover, in \cite{beyer1999nearest}...
\section{Properties of High Dimensional Spaces}
I'm not sure what we should put here. The only reference we have for this part are Haralick's slides.

\

-distance between spaces decreases\\
-volume shrinks\\
-shell pushed to boundary\\
-bounding box volume pushed to corners

\
Here we should reference the specific results in \cite{aggarwal2001surprising}, then talk about our ``motivation" for confirming the results.
\section{Experimental Results}
As explained in the previous section, we wish to verify experimentally two effects of high dimension spaces, namely the distance between points and the effect on classification accuracy. 
\subsection{Distance Ratio}
To explore the distance between a set of $K$ points, we focus on the average ratio $r$ of the minimum and maximum distances between all $K$ points. We explore how the following parameters affect $r$:
\begin{itemize}
\item $N$ - the dimension of the space, taking values from $[1,10)\cup[10,20,\ldots,100]$
\item $K$ - the samples space, taking values in $\{5,50,500\}$
\item $L_p$, - the $p-$norm, with $p\in\{1,2,\infty\}$
\end{itemize}
\subsubsection{Uniform Distribution}
In this set of experiments, each of the $N$ components of the $K$ points were drawn from the uniform distribution on the interval $(0,1)$. Figure \ref{fig:exp11} depicts the results for each sample size $K$.
\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{0.45\textwidth}
        \includegraphics[width=\textwidth]{l-experiment1-1-5-inf.png}
        \caption{$K=5$}
        \label{fig:exp1k5}
    \end{subfigure}
   \begin{subfigure}[h]{0.45\textwidth}
        \includegraphics[width=\textwidth]{l-experiment1-1-50-inf.png}
        \caption{$K=50$}
        \label{fig:exp11k50}
    \end{subfigure}
\end{figure}
\begin{figure}[H]
   \centering
   \ContinuedFloat 
    \begin{subfigure}[h]{0.9\textwidth}
        \includegraphics[width=\textwidth]{l-experiment1-1-500-inf.png}
        \caption{$K=500$}
        \label{fig:exp11k500}
    \end{subfigure}
    \caption{Average distance ratio $r$ of $K$ samples for metrics $L_1,L_2,$ and $L_{\infty}$, Uniform Distribution. Standard deviation bars are depicted for each metric and dimension.}\label{fig:exp11}
\end{figure}
As evinced by the large standard deviations, there is a high degree of noise for the smallest sample size ($K=5$),  with no metric being consistently better than the other even as the dimension is increased. However, by $K=50$ a clear trend has formed, with each lower-valued $p$-metric performing better than those with greater value. Notice that the noise of the data has also been reduced. At $K=500$ samples the results are the same, again with less noise that the previous two sample sets.
\
\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Metric & $N$ & $r$ & \% Difference\\
\hline
1 & 1 & 0.00139 & N/A\\
2 & 1 & 0.00141 & 1.37\%\\
$\infty$ & 1 & 0.00152 & 9.26\%\\
\hline
1 & 10 & 0.25019 & N/A\\
2 & 10 & 0.28958 & 15.74\%\\
$\infty$ & 10 & 0.32590 & 30.26\%\\
\hline
1 & 100 & 0.66599 & N/A\\
2 & 100 & 0.71255 & 6.99\%\\
$\infty$ & 100 & 0.76011 & 14.13\%\\
\hline
\end{tabular}
\caption{Average distance ratios for $K=500$, comparing each norm to $L_1$.}
\label{fig:exp11data}
\end{figure}
In Figure \ref{fig:exp11data}, we see a comparison of the average distance ratios for each metric on the $K=500$ data set. At $N=1$, $r$ is on the order of $10^{-3}$, with the $L_1$ and $L_{\infty}$ metrics performing within $9\%$ of each other. At $N=100$, $r$ varies between $2/3$ and roughly $3/4$, with a $14\%$ difference between $L_1$ and $L_{\infty}$.
\subsubsection{Normal Distribution}
In this set of experiments, each of the $N$ components of the $K$ points were drawn from a normal distribution with mean 0 and variance 1. Figure \ref{fig:exp2} depicts the results for each sample size $K$.
\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{0.45\textwidth}
        \includegraphics[width=\textwidth]{l-experiment2-5-inf.png}
        \caption{$K=5$}
        \label{fig:exp2k5}
    \end{subfigure}
   \begin{subfigure}[h]{0.45\textwidth}
        \includegraphics[width=\textwidth]{l-experiment2-50-inf.png}
        \caption{$K=50$}
        \label{fig:exp2k50}
    \end{subfigure}
   \centering
\end{figure}
\begin{figure}[H]
\centering
   \ContinuedFloat 
    \begin{subfigure}[h]{0.9\textwidth}
        \includegraphics[width=\textwidth]{l-experiment2-500-inf.png}
        \caption{$K=500$}
        \label{fig:exp2k500}
    \end{subfigure}
    \caption{Average distance ratio $r$ of $K$ samples for metrics $L_1,L_2,$ and $L_{\infty}$, normal distribution with mean 0 and variance 1. Standard deviation bars are depicted for each metric and dimension.}\label{fig:exp2}
\end{figure}
As evinced by the large standard deviations, there is a high degree of noise for the smallest sample size ($K=5$),  with no metric being consistently better than the other even as the dimension is increased. Note that due to the normal distribution, we get a standard deviation range which is negative for small values of $N$. However, by $K=50$ a clear trend has formed, with $L_1$ performing better than $L_2$, and with $L_{\infty}$ having the best performance. At $K=500$ samples the results are the same, again with less noise that the previous two sample sets.
\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Metric & $N$ & $r$ & \% Difference\\
\hline
1 & 1 & 0 & 0\\
2 & 1 & 0 & 0\\
$\infty$ & 1 & 0 & N/A\\
\hline
1 & 10 & 0.30827 & 6.22\%\\
2 & 10 & 0.32209 & 10.98\%\\
$\infty$ & 10 & 0.29021 & N/A\\
\hline
1 & 100 & 0.70784 & 45.62\%\\
2 & 100 & 0.73351 & 50.90\%\\
$\infty$ & 100 & 0.48610 & N/A\\
\hline
\end{tabular}
\caption{Average distance ratios for $K=500$, comparing each norm to $L_{\infty}$.}
\label{fig:exp2data}
\end{figure}
In Figure \ref{fig:exp2data}, we see a comparison of the average distance ratios for each metric on the $K=500$ data set. At $N=1$, $r=0$ for all metrics. At $N=100$, $r$ varies between $7/10$ and roughly $1/2$, with a $50\%$ difference between $L_2$ and $L_{\infty}$.

\subsection{Classification Accuracy}
-setup\\
-synthetic data set\\
-tests that were run\\
-results (conf matrix) as N is varied\\
-results (conf matrix) as K is varied\\
-results (conf matrix) as $\sigma$ is varied
-how different metrics affect these results
\section{Conclusion}
-summarize results\\
-suggest other experiments (fractional distance metric, other clustering, other ML techniques which use distance?
	
\bibliographystyle{plain}
\bibliography{ml-midterm}

\end{document}  