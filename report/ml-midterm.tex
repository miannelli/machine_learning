\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}

\title{The Ramifications of High Dimensional Spaces on Machine Learning Techniques}
\author{Jonathan Gryak
\qquad
Michael Iannelli\\
PhD Program in Computer Science\\
CUNY Graduate Center\\
City University of New York\\
\texttt{\{jgryak, miannelli\}@gradcenter.cuny.edu}}
\date{}
\begin{document}

\maketitle

\begin{abstract}
Stuff
\end{abstract}
\tableofcontents
\section{Introduction}
motivation:\\
-nearest neighbor\\
-k-means\\
-similarity indexing\\
need more references for the use of these in high dimensions\\
in the introduction to machine learning book, it speaks of dimensionality with respect to density estimation,
not sure if that's useful here

\

The ``curse of dimensionality" coined by Richard Bellman in 1961\cite{bellman1961adaptive}, comes in many forms, including Bellman's original context of optimization, functional approximation, and combinatorics. In this era of ``big data", ``big" not only refers to the indefatigable increase the amount of data collected, but also in the number of features and components that are utilized in the data analysis. However,  as shown in\cite{aggarwal2001surprising}, the increase in the number of features considered, which are usually considered as a multi-dimensional space, has grave repercussions for machine learning techniques that rely on some measure of distance between points in the feature space. Moreover, in \cite{beyer1999nearest}...
\section{Properties of High Dimensional Spaces}
I'm not sure what we should put here. The only reference we have for this part are Haralick's slides.

\

-distance between spaces decreases\\
-volume shrinks\\
-shell pushed to boundary\\
-bounding box volume pushed to corners

\
Here we should reference the specific results in \cite{aggarwal2001surprising}, then talk about our ``motivation" for confirming the results.
\section{Experimental Results}
As explained in the previous section, we wish to verify experimentally two effects of high dimension spaces, namely the distance between points and the effect on classification accuracy. 
\subsection{Distance Ratio}
To explore the distance between a set of $K$ points, we focus on the average ratio $r$ of the minimum and maximum distances between all $K$ points. We explore how the following parameters affect $r$:
\begin{itemize}
\item $N$ - the dimension of the space, taking values from $[1,10)\cup[10,20,\ldots,100]$
\item $K$ - the samples space, taking values in $\{5,50,500\}$
\item $L_p$, - the $p-$norm, with $p\in\{1,2,\infty\}$
\end{itemize}
\subsubsection{Uniform Distribution}
In this set of experiments, each of the $N$ components of the $K$ points were drawn from the uniform distribution on the interval $(0,1)$. Figure \ref{fig:exp11} depicts the results for each sample size $K$.
\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{0.45\textwidth}
        \includegraphics[width=\textwidth]{l-experiment1-1-5-inf.png}
        \caption{$K=5$}
        \label{fig:exp1k5}
    \end{subfigure}
   \begin{subfigure}[h]{0.45\textwidth}
        \includegraphics[width=\textwidth]{l-experiment1-1-50-inf.png}
        \caption{$K=50$}
        \label{fig:exp11k50}
    \end{subfigure}
\end{figure}
\begin{figure}[H]
   \centering
   \ContinuedFloat 
    \begin{subfigure}[h]{0.9\textwidth}
        \includegraphics[width=\textwidth]{l-experiment1-1-500-inf.png}
        \caption{$K=500$}
        \label{fig:exp11k500}
    \end{subfigure}
    \caption{Average distance ratio $r$ of $K$ samples for metrics $L_1,L_2,$ and $L_{\infty}$, Uniform Distribution. Standard deviation bars are depicted for each metric and dimension.}\label{fig:exp11}
\end{figure}
As evinced by the large standard deviations, there is a high degree of noise for the smallest sample size ($K=5$),  with no metric being consistently better than the other even as the dimension is increased. However, by $K=50$ a clear trend has formed, with each lower-valued $p$-metric performing better than those with greater value. Notice that the noise of the data has also been reduced. At $K=500$ samples the results are the same, again with less noise that the previous two sample sets.
\
\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Metric & $N$ & $r$ & \% Difference\\
\hline
1 & 1 & 0.00139 & N/A\\
2 & 1 & 0.00141 & 1.37\%\\
$\infty$ & 1 & 0.00152 & 9.26\%\\
\hline
1 & 10 & 0.25019 & N/A\\
2 & 10 & 0.28958 & 15.74\%\\
$\infty$ & 10 & 0.32590 & 30.26\%\\
\hline
1 & 100 & 0.66599 & N/A\\
2 & 100 & 0.71255 & 6.99\%\\
$\infty$ & 100 & 0.76011 & 14.13\%\\
\hline
\end{tabular}
\caption{Average distance ratios for $K=500$, comparing each norm to $L_1$.}
\label{fig:exp11data}
\end{figure}
In Figure \ref{fig:exp11data}, we see a comparison of the average distance ratios for each metric on the $K=500$ data set. At $N=1$, $r$ is on the order of $10^{-3}$, with the $L_1$ and $L_{\infty}$ metrics performing within $9\%$ of each other. At $N=100$, $r$ varies between $2/3$ and roughly $3/4$, with a $14\%$ difference between $L_1$ and $L_{\infty}$.
\subsubsection{Normal Distribution}
In this set of experiments, each of the $N$ components of the $K$ points were drawn from a normal distribution with mean 0 and variance 1. Figure \ref{fig:exp2} depicts the results for each sample size $K$.
\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{0.45\textwidth}
        \includegraphics[width=\textwidth]{l-experiment2-5-inf.png}
        \caption{$K=5$}
        \label{fig:exp2k5}
    \end{subfigure}
   \begin{subfigure}[h]{0.45\textwidth}
        \includegraphics[width=\textwidth]{l-experiment2-50-inf.png}
        \caption{$K=50$}
        \label{fig:exp2k50}
    \end{subfigure}
   \centering
\end{figure}
\begin{figure}[H]
\centering
   \ContinuedFloat 
    \begin{subfigure}[h]{0.9\textwidth}
        \includegraphics[width=\textwidth]{l-experiment2-500-inf.png}
        \caption{$K=500$}
        \label{fig:exp2k500}
    \end{subfigure}
    \caption{Average distance ratio $r$ of $K$ samples for metrics $L_1,L_2,$ and $L_{\infty}$, normal distribution with mean 0 and variance 1. Standard deviation bars are depicted for each metric and dimension.}\label{fig:exp2}
\end{figure}
As evinced by the large standard deviations, there is a high degree of noise for the smallest sample size ($K=5$),  with no metric being consistently better than the other even as the dimension is increased. Note that due to the normal distribution, we get a standard deviation range which is negative for small values of $N$. However, by $K=50$ a clear trend has formed, with $L_1$ performing better than $L_2$, and with $L_{\infty}$ having the best performance. At $K=500$ samples the results are the same, again with less noise that the previous two sample sets.
\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Metric & $N$ & $r$ & \% Difference\\
\hline
1 & 1 & 0 & 0\\
2 & 1 & 0 & 0\\
$\infty$ & 1 & 0 & N/A\\
\hline
1 & 10 & 0.30827 & 6.22\%\\
2 & 10 & 0.32209 & 10.98\%\\
$\infty$ & 10 & 0.29021 & N/A\\
\hline
1 & 100 & 0.70784 & 45.62\%\\
2 & 100 & 0.73351 & 50.90\%\\
$\infty$ & 100 & 0.48610 & N/A\\
\hline
\end{tabular}
\caption{Average distance ratios for $K=500$, comparing each norm to $L_{\infty}$.}
\label{fig:exp2data}
\end{figure}
In Figure \ref{fig:exp2data}, we see a comparison of the average distance ratios for each metric on the $K=500$ data set. At $N=1$, $r=0$ for all metrics. At $N=100$, $r$ varies between $7/10$ and roughly $1/2$, with a $50\%$ difference between $L_2$ and $L_{\infty}$.

\subsection{Classification Accuracy}
\paragraph{}
In \cite{aggarwal2001surprising}, the impact of different norms on classification accuracy was investigated, where it was demonstrated that, with respect to a random classification, lower valued norms performed better. In this section, we investigate how classification accuracy varies with respect to dimension, sample size, and and data variance. The parameters used in these experiments are:
\begin{itemize}
\item $N$ - the dimension of the space, taking values from $[1,10)\cup[10,20,\ldots,100]$
\item $K$ - the samples space, taking values in $\{10,300\}$ in increments of 10
\item $M$ - the number of points in the data set to be classified
\item $\sigma$ - the standard deviation of a normal distribution, with $\sigma\in\{.05,.1,.15,.2,.25\}$
\end{itemize}
\paragraph{}
For each experiment, a labeled set $X$ of $K$ $N$-dimensional sample points was generated, with the component of each point being drawn from a uniform distribution on $(0,1)$, and each point being assigned a random, binary classification (i.e., 0 or 1). Then, a data set $Y$ of $M$ $N$-dimensional points is generated with components drawn  from a uniform distribution on $(0,1)$. Each point in $Y$ is given the classification by it nearest neighbor in $X$. The new dataset $\tilde{Y}$ is created by taking each point in $Y$ and perturbing each of its components by adding a value drawn from a normal distribution with mean $0$ and variance $\sigma$. The dataset $\tilde{Y}$ is then classified in the same way as $Y$, and the confusion matrix of between $Y$ and $\tilde{Y}$ is then generated.
For each combination of $N$, $K$, and $\sigma$ values, ten trials of the above experiment were performed. The results of the trials were then averaged. The standard Euclidean norm ($\mathcal{L}_2$) was used in all experiments throughout this section. Note that we also used a different measure of accuracy than in \cite{aggarwal2001surprising}. Accuracy in this section is defined as:
$$
	\dfrac{\#\textrm{true positives}+\#\textrm{true negatives}}{\#\textrm{total outcomes}}
$$
\subsubsection{The Effect of Dimension on Accuracy}
\begin{figure}[H]
    \centering
        \includegraphics[width=\textwidth]{l-experiment1-2-accuracy-n-k-300-s-005.png}
    \caption{Accuracy as $N$ Varies, $K=300$, $\sigma=.05$}\label{fig:exp1-2-accuracy-n}
\end{figure}
\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$N$&\multicolumn{2}{|c|}{Confusion Matrix}&Accuracy\\
\hline
1&27.9&24.7&0.504\\
\cline{2-3}
&24.9&22.5&\\
\hline
10&35.8&13.9&	0.689\\
\cline{2-3}
&17.2&33.1&\\
\hline
100&35&14.1&0.707\\
\cline{2-3}
&15.2&35.7&\\
\hline
\end{tabular}
\caption{Confusion Matrices for Figure \ref{fig:exp1-2-accuracy-n}}\label{fig:cm-exp1-2-accuracy-n}
\end{figure}
Our experiments showed a positive relationship between accuracy and dimension. From the results depicted in Figures \ref{fig:exp1-2-accuracy-n} and \ref{fig:cm-exp1-2-accuracy-n}, we see that accuracy starts at approximately 50\%, climbs rapidly to approximately 70\%, the remains at that level as the number of dimensions increases. Lower samples size and higher variance increased the amount of noise in the data but did not produce contradicting results.
\subsubsection{The Effect of Sample Size on Accuracy}
\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{0.45\textwidth}
        \includegraphics[width=\textwidth]{l-experiment1-2-accuracy-k-n-10-s-005.png}
        \caption{$N=10$}
        \label{fig:exp2k1}
    \end{subfigure}
   \begin{subfigure}[h]{0.45\textwidth}
        \includegraphics[width=\textwidth]{l-experiment1-2-accuracy-k-n-100-s-005.png}
        \caption{$N=100$}
        \label{fig:exp2k2}
    \end{subfigure}
    \caption{Accuracy as $K$ Varies, $N$ as labeled, $\sigma=.05$}\label{fig:exp1-2-accuracy-k}
\end{figure}
\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
$K$&\multicolumn{2}{|c|}{Confusion Matrix}&Accuracy\\
\hline
10&47.3&12.9&0.777\\
\cline{2-3}
&9.4&30.4&\\
\hline
100&32.8&15.6&0.716\\
\cline{2-3}
&12.8&38.8&\\
\hline
300&35&14.1&0.707\\
\cline{2-3}
&15.2&35.7&\\
\hline
\end{tabular}
\caption{Confusion Matrices for Figure \ref{fig:exp1-2-accuracy-k}}\label{fig:cm-exp1-2-accuracy-k}
\end{figure}
Our experiments showed a negative relationship between accuracy and sample size, which weakened as $N$ increased. In Figure \ref{fig:exp2k1} for $N=10$ we see a drop by .10 in the accuracy over the range of $K$, whereas in figure {fig:exp2k2} for $N=100$ the same range in sample size exhibits only half that reduction in accuracy.
\subsubsection{The Effect of Perturbation Variance on Accuracy}
\begin{figure}[H]
    \centering
        \includegraphics[width=\textwidth]{experiment1-2-sigmas.png}
    \caption{Accuracy as $\sigma$ Varies, $N=100$,$K=300$}\label{fig:exp1-2-accuracy-s}
\end{figure}
Our initial range of $\sigma$  did not reveal any significant correlation with accuracy, so the range of $\sigma$ was expanded from .05  to .95 inclusively in .05 increments, keeping $N$ and $K$ constant at 100 and 300 respectively. As before, 10 trials were run for each $\sigma$ value, and the results for each were averaged. Even in this expanded range, the accuracy oscillated as $\sigma$ increased, staying with a range of .56 to .60.
\subsubsection{On the Number of Classes and Accuracy}

\section{Conclusion}
-summarize results\\
-suggest other experiments (fractional distance metric, other clustering, other ML techniques which use distance?
	
\bibliographystyle{plain}
\bibliography{ml-midterm}

\end{document}  