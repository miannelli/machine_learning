\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{cleveref}
\usepackage{multirow}
\newcommand{\Z}{\mathbb{Z}}

\newcommand{\N}{\mathbb{N}}

\newcommand{\Q}{\mathbb{Q}}

\newcommand{\R}{\mathbb{R}}

\newcommand{\C}{\mathbb{C}}

\title{The Ramifications of High Dimensional Spaces on Machine Learning Techniques}
\author{Jonathan Gryak
\qquad
Michael Iannelli\\
PhD Program in Computer Science\\
CUNY Graduate Center\\
City University of New York\\
\texttt{\{jgryak, miannelli\}@gradcenter.cuny.edu}}
\date{}
\begin{document}

\maketitle

\begin{abstract}
In the age of big data, the analysis of features in many real-world applications requires the use of high dimensional spaces. Many machine learning techniques, including similarity indexing, density estimation, and nearest neighbors, use the proximity of points in $N$-dimensional as the primary means of classification. However, the geometry of high dimensional spaces and the metrics used to measure distance reduces the efficacy of these techniques. Building upon the work of Beyer, Aggarwal, Haralick, and others, we provide experimental results concerning the performance of $\mathcal{L}_k$-based metrics with respect to dimension, sample size, and the classification accuracy of nearest neighbors. For points drawn from uniform distributions, lower values of $k$ perform better, whereas $\mathcal{L}_{\infty}$ performs best for normally distributed points.  Regardless of the choice of metric, the distance between points diminishes as dimension increases. Classification accuracy, however, decreases with respect to dimension. We also show that classification accuracy decreases as the cardinality of the class set increases.
\end{abstract}
\tableofcontents
\section{Introduction}
\paragraph{}
The ``curse of dimensionality" originated with Richard Bellman in 1961\cite{bellman1961adaptive} in the context of optimization. Since then it has been applied to the difficulty of working in higher dimensional spaces in fields such as function approximation, combinatorics, and machine learning. In this era of ``big data", ``big" not only refers to the inexorable increase in the amount of data collected, but also in the number of features and components that are utilized in the data analysis.  
\paragraph{}
It is common in the area of genomics and computational biology for a problem to have more features than observations.  Such is the case in gene expression arrays where where a data set can have thousands of genes, yet less than a hundred samples \cite{hastie01statisticallearning}.  Other examples of high dimensional data can be found in recommender systems, such as those found at Amazon or Netflix where customers may have rated up to thousands of products.  In 2006, Netflix released an anonymized data set consisting of the ratings of a half-million customers on nearly 18,000 movies\cite{Bennett:2007:KCW:1345448.1345459}.
\paragraph{}
The explosion of features in modern big data is not without its challenges.  As shown in \cite{aggarwal2001surprising}, the increase in the number of features considered, which are usually considered as a multi-dimensional space, has grave repercussions for machine learning techniques that rely on a measure of distance between points in the feature space. Common measures such as Euclidean distance no longer provide sufficient discrimination between points,
\paragraph{}
In this paper, we will provide an overview of the novel geometry of high dimensional spaces. Expanding upon the results of \cite{aggarwal2001surprising}, we will explore the effects of dimension, sample size, and metric choice on distances in high dimensional space. Utilizing the nearest neighbor technique, we also explore the repercussions of these choices on classification accuracy. Our experiments show that regardless of the choice of metric, the average ratio of maximum distances to minimum distances among points in the space increases as the number of dimensions increase. For points drawn from uniform distributions, metrics induced by norms $\mathcal{L}_k$ with lower $k$ values perform better than higher ones, whereas for normally distributed points the max norm  $\mathcal{L}_{\infty}$ performs the best. We show that classification accuracy increases as dimension does, but is negatively affected by increases in the cardinality of the set of classes.

\section{Properties of High Dimensional Spaces}
\subsection{The Geometry of High Dimensional Spaces}
In \cite{haralickgeometryhd}, Haralick outlines the geometry of bounded, high dimensional spaces and its effect on volume and distance. We summarize these results below:
\begin{itemize} 
\item\emph{Volume of a Sphere} - In high dimensional spaces, the volume of a sphere of fixed radius $r$ with dimension $n$ tends to zero as $n$ increases, specifically
$$
\lim_{n\rightarrow\infty}\dfrac{\pi^{n/2}r^n}{\Gamma(d/2+1)}=0,
$$
with $\Gamma$ being the gamma function.
\item \emph{Volume Contained in Shell} - For any hypersphere in $n$ dimensions of radius $r$, let
$$
f(n,\Delta r)=1 -\dfrac{r^n}{(r+\Delta r)^n}
$$
be the fraction of a volume contained in a shell of width $\Delta r$. For a fixed width, $f(n,\Delta r)$ tends to 1 as $n$ increases, thus all of the volume of the sphere is pushed into its shell.
\item\emph{Bounding Hypercube} - For any hypercube bounding a hypersphere in high dimensional space, the ratio of the volume of the bounding hypercube to the volume of its enclosed hypersphere approaches zero as dimension increases, with less that 10\% of the hypercube's volume contained in the hypersphere for dimensions $\geq 6$. This suggest that all points in the bounding hypercube are pushed into its corners. 
\item\emph{Maximum and Minimum Distances} - In \cite{haralicknn}, it is shown that for any point in the space, the maximum and minimum distances to any other point in the space increase as the dimension of the space does, and that the ratio of the two approaches unity.  This evinces the poor discrimination distance provides among points in high dimension.
\end{itemize}
For machine learning techniques that rely upon the distance or density of space, the geometry suggests that naively extrapolating these techniques to higher dimensions will produce poor results.
\subsection{The Implications of Metric Choice}
\paragraph{}
In \cite{aggarwal2001surprising}, the authors investigate this peculiar geometry through the lens of $\mathcal{L}_k$ norms. For $k\in\Z$, the norm of two vectors $x,y\in\R^n$ is defined as
$$
\mathcal{L}_k=\left(\sum_{i=1}^n\|x\|^k\right)^{1/k}.
$$
Note that the limit of this norm is called the $\mathcal{L}_{\infty}$ or max norm, and is defined as
$$
\mathcal{L}_k=\max(|x_1|,\ldots,|x_n|).
$$
The authors make use of a ratio called the \emph{relative contrast}, defined as
$$
\dfrac{Dmax_n^k-Dmin_n^k}{Dmin_n^k},
$$
where $Dmax$ and $Dmin$ are respectively the furthest and closest points to the origin in a data set under the metric induced by $\mathcal{L}_k$. The authors extend work by Beyer\cite{beyer1999nearest} to show that, irrespective of the distribution the points are drawn from, $Dmax_n^k-Dmin_n^k$ increases at a rate of $n^{1/k-1/2}$. This has implications for how each  $\mathcal{L}_k$ performs in high dimensions. Generally speaking, the higher the value of $k$ the less distance can be used to discriminate among points.
\paragraph{}
In addition to testing standard $\mathcal{L}_k$ norms,  Aggarwal, et al. investigate fractional norms, where $0< k < 1$. For these values of $k$, the relative contrast remains a valid measure in higher dimensions that integral values. Classification accuracy for the $l$-nearest neighbor algorithm is tested on both synthetic and real datasets. The authors define accuracy as the total number true positives classified. It is demonstrated that fractional metrics are the most accurate when comparison to a random classification of the data.
\paragraph{}
In this paper, we wish to experimentally verify these counterintuitive effects. We will perform experiments to determine the effects of dimension, metric choice, and sample size on the distances between points. We will also experiment with drawing points from different distributions. Moreover, we will also explore the effects of these parameters on classification accuracy. 
\section{Experimental Results}
As explained in the previous section, we wish to verify experimentally two effects of high dimension spaces, namely the distance between points and the effect on classification accuracy. 
\subsection{Distance Ratio}
To explore the distance between a set of $K$ points, we focus on the average ratio $r$ of the minimum and maximum distances between all $K$ points. We explore how the following parameters affect $r$:
\begin{itemize}
\item $N$ - the dimension of the space, taking values from $[1,10)\cup[10,20,\ldots,100]$
\item $K$ - the samples space, taking values in $\{5,50,100,500,1000,5000\}$
\item $L_p$, - the $p-$norm, with $p\in\{1,2,\infty\}$
\end{itemize}
\subsubsection{Uniform Distribution}
In this set of experiments, each of the $N$ components of the $K$ points were drawn from the uniform distribution on the interval $(0,1)$. Figure \ref{fig:exp11} depicts the results for each sample size $K$.
\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{0.75\textwidth}
        \includegraphics[width=\textwidth]{experiment1-1-100.png}
        \caption{$K=100$}
        \label{fig:exp1k100}
    \end{subfigure}
   \begin{subfigure}[h]{0.75\textwidth}
        \includegraphics[width=\textwidth]{experiment1-1-500.png}
        \caption{$K=500$}
        \label{fig:exp11k500}
    \end{subfigure}
\end{figure}
\begin{figure}[H]
   \centering
   \ContinuedFloat 
    \begin{subfigure}[h]{0.75\textwidth}
        \includegraphics[width=\textwidth]{experiment1-1-1000.png}
        \caption{$K=1000$}
        \label{fig:exp1k1000}
    \end{subfigure}
    \begin{subfigure}[h]{0.75\textwidth}
        \includegraphics[width=\textwidth]{experiment1-1-5000.png}
        \caption{$K=5000$}
        \label{fig:exp11k5000}
    \end{subfigure}
    \caption{Average distance ratio $r$ of $K$ samples for metrics $L_1,L_2,$ and $L_{\infty}$, Uniform Distribution. Standard deviation bars are depicted for each metric and dimension.}\label{fig:exp11}
\end{figure}
For all the depicted sample sizes a clear trend is visible, with each lower-valued $p$-metric performing better than those with greater value. Notice that the noise of the data, as shown by the standard deviation, decreases as $K$ increases.
\
\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Metric & $N$ & $r$ & \% Difference\\
\hline
1 & 1 & 0.00014 & N/A\\
2 & 1 & 0.00014 & 0.12\%\\
$\infty$ & 1 & 0.00014 & 0.48\%\\
\hline
1 & 10 & 0.17764 & N/A\\
2 & 10 & 0.21229 & 19.51\%\\
$\infty$ & 10 & 0.24724 & 39.18\%\\
\hline
1 & 100 & 0.60767 & N/A\\
2 & 100 & 0.66264 & 9.05\%\\
$\infty$ & 100 & 0.71780 & 18.12\%\\
\hline
\end{tabular}
\caption{Average distance ratios for $K=5000$, comparing each norm to $L_1$.}
\label{fig:exp11data}
\end{figure}
In Figure \ref{fig:exp11data}, we see a comparison of the average distance ratios for each metric on the $K=5000$ data set. At $N=1$, $r$ is on the order of $10^{-4}$, but by $N=100$, $r$ has grown to approximately .76.
\subsubsection{Normal Distribution}
In this set of experiments, each of the $N$ components of the $K$ points were drawn from a normal distribution with mean 0 and variance 1. The points were normalized using the $\mathcal{L}_2$ norm, which results in the vectors being uniformly distributed on the unit hypersphere. Figure \ref{fig:exp2} depicts the results for each sample size $K$.
\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{.8\textwidth}
        \includegraphics[width=\textwidth]{l-experiment2-5-inf.png}
        \caption{$K=5$}
        \label{fig:exp2k5}
    \end{subfigure}
\end{figure}
\begin{figure}[H]
\centering
   \ContinuedFloat
   \begin{subfigure}[h]{.8\textwidth}
        \includegraphics[width=\textwidth]{l-experiment2-50-inf.png}
        \caption{$K=50$}
        \label{fig:exp2k50}
    \end{subfigure}
   \centering
\end{figure}
\begin{figure}[H]
\centering
   \ContinuedFloat 
    \begin{subfigure}[h]{.8\textwidth}
        \includegraphics[width=\textwidth]{l-experiment2-500-inf.png}
        \caption{$K=500$}
        \label{fig:exp2k500}
    \end{subfigure}
    \caption{Average distance ratio $r$ of $K$ samples for metrics $L_1,L_2,$ and $L_{\infty}$, normal distribution with mean 0 and variance 1. Standard deviation bars are depicted for each metric and dimension.}\label{fig:exp2}
\end{figure}
As with the uniform distribution, the trend in the performance of the various norms is smoothed as $K$ increases. Also in common with the previous experiment, the $\mathcal{L}_1$ performs better than the $\mathcal{L}_2$ norm. However, it is the $\mathcal{L}_{\infty}$ norm which performs best as $N$ increases.
\

In order to isolate the difference in performance, we first repeated the experiment but varying the normalization. While normalizing using a different norm did change the specific means, it did not change the overall performance or pattern as shown in \cref{fig:normmetric-1,fig:normmetric-2,fig:normmetric-inf}. Nor did repeating the experiment without normalization as shown in \cref{fig:no_normalization}. We then turned our attention to how the $\mathcal{L}_{\infty}$ norm affects points drawn from a uniform or normal distribution differently.
\
\begin{figure}[H]
\centering
\begin{subfigure}[h]{0.75\textwidth}
        \includegraphics[width=1\textwidth]{normalization_metric-1.png}
        \caption{Average distance ratio $r$ for each distance metric (columns) varying over dimension of space $N$, normalized by $\mathcal{L}_1$}
        \label{fig:normmetric-1}
\end{subfigure}
\begin{subfigure}[h]{0.75\textwidth}
        \includegraphics[width=1\textwidth]{normalization_metric-2.png}
        \caption{Average distance ratio $r$ for each distance metric (columns) varying over dimension of space $N$, normalized by $\mathcal{L}_2$}
        \label{fig:normmetric-2}
\end{subfigure}
\begin{subfigure}[h]{0.75\textwidth}
        \includegraphics[width=1\textwidth]{normalization_metric-inf.png}
        \caption{Average distance ratio $r$ for each distance metric (columns) varying over dimension of space $N$, normalized by $\mathcal{L}_{\infty}$}
        \label{fig:normmetric-inf}
\end{subfigure}
\end{figure}
\begin{figure}[H]
\centering
   \ContinuedFloat 
\begin{subfigure}[h]{0.75\textwidth}
        \includegraphics[width=1\textwidth]{exp2without_normalization.png}
        \caption{Average distance ratio for metrics $L_1,L_2,$ and $L_{\infty}$, non-normalized points whose components are drawn from a normal distribution with mean 0 and variance 1.}
        \label{fig:no_normalization}
        \end{subfigure}
        \caption{The Effect of Normalization on Distance Ratio $r$}
\end{figure}

For a vector whose components are drawn from a uniform distribution on $(0, 1)$ or have been normalized, $\mathcal{L}_{\infty}$ is bounded above by 1.
\paragraph{}
Displayed below is the average variance of the $\mathcal{L}_{\infty}$ norms of 1000 vectors whose components are randomly drawn from uniform and normal distributions respectively, plotted against the arity of the vector.  The variance decreases to approximately 0 for the uniform distribution, while still remaining relatively high for the normal distribution:
\begin{figure}[H]
\centering
\begin{subfigure}[h]{0.7\textwidth}
        \includegraphics[width=.75\textwidth]{exp2-var-uniform.png}
        \caption{Average Variance of $\mathcal{L}_{\infty}$, Uniform Distribution}
        \label{fig:exp2varuniform}
\end{subfigure}
\end{figure}
\begin{figure}[H]
\centering
   \ContinuedFloat 
\begin{subfigure}[h]{0.7\textwidth}
        \includegraphics[width=.75\textwidth]{exp2-var-norm.png}
        \caption{Average Variance of $\mathcal{L}_{\infty}$, Normal Distribution}
        \label{fig:exp2varnorm}
        \end{subfigure}
        \caption{Average Variance of $\mathcal{L}_{\infty}$}
\end{figure}
By its use of the maximum component of the vector, $\mathcal{L}_{\infty}$ is good at capturing outlying data. Due to the geometry of high dimensional spaces, outlying values become the defining points of our data. This, combined with the variance of $\mathcal{L}_{\infty}$ of vectors drawn from the normal distributions, may explain these counterintuitive results.

\subsection{Classification Accuracy}
\paragraph{}
In \cite{aggarwal2001surprising}, the impact of different norms on classification accuracy was investigated, where it was demonstrated that, with respect to a random classification, lower valued norms performed better. In this section, we investigate how classification accuracy varies with respect to dimension, sample size, norm, and data variance. The parameters used in these experiments are:
\begin{itemize}
\item $N$ - the dimension of the space, taking values from $[1,10)\cup[10,20,\ldots,100]$
\item $K$ - the number of samples, taking values in $\{10,300\}$ in increments of 10
\item $M$ - the number of points in the data set to be classified
\item $L_p$, - the $p-$norm, with $p\in\{1,2,\infty\}$
\item $\sigma$ - the standard deviation of a normal distribution, with $\sigma\in\{.05,.1,.15,.2,.25\}$
\end{itemize}
\paragraph{}
For each experiment, a labeled set $X$ of $K$ $N$-dimensional sample points was generated, with the component of each point being drawn from a uniform distribution on $(0,1)$, and each point being assigned a random, binary classification (i.e., 0 or 1). Then, a data set $Y$ of $M$ $N$-dimensional points is generated with components drawn  from a uniform distribution on $(0,1)$. Each point in $Y$ is given the classification by it nearest neighbor in $X$. The new dataset $\tilde{Y}$ is created by taking each point in $Y$ and perturbing each of its components by adding a value drawn from a normal distribution with mean $0$ and variance $\sigma$. The dataset $\tilde{Y}$ is then classified in the same way as $Y$, and the confusion matrix of between $Y$ and $\tilde{Y}$ is then generated.
For each combination of $N$, $K$, $L_p$, and $\sigma$ values, ten trials of the above experiment were performed. The results of the trials were then averaged. Note that we also used a different measure of accuracy than in \cite{aggarwal2001surprising}. Accuracy in this section is defined as:
$$
	\dfrac{\#\textrm{true positives}+\#\textrm{true negatives}}{\#\textrm{total outcomes}}
$$
\subsubsection{The Effect of Dimension on Accuracy}
\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{.75\textwidth}
        \includegraphics[width=\textwidth]{l-experiment1-2-accuracy-n-k-300-s-005-l1.png}
    	\caption{$\mathcal{L}_1$}
    	\label{fig:exp1-2-accuracy-n-1}
    \end{subfigure}
    \begin{subfigure}[h]{.75\textwidth}
        \includegraphics[width=\textwidth]{l-experiment1-2-accuracy-n-k-300-s-005-l2.png}
    	\caption{$\mathcal{L}_2$}
    	\label{fig:exp1-2-accuracy-n-2}
    \end{subfigure}
\end{figure}
\begin{figure}[H]
\centering
   \ContinuedFloat 
    \begin{subfigure}[h]{.75\textwidth}
    	\includegraphics[width=\textwidth]{l-experiment1-2-accuracy-n-k-300-s-005-linf.png}
    	\caption{$\mathcal{L}_{\infty}$}
	\label{fig:exp1-2-accuracy-n-inf}
    \end{subfigure}
    	\caption{Accuracy as $N$ Varies, $K=300$, $\mathcal{L}$ as labeled,, $\sigma=.05$}
    \label{fig:exp1-2-accuracy-n}
\end{figure}
\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
$\mathcal{L}$&$N$&\multicolumn{2}{|c|}{Confusion Matrix}&Accuracy\\
%metric 1
\hline
\multirow{6}{*}{$\mathcal{L}_1$}&\multirow{2}{*}{1}&28.9&26.2&\multirow{2}{*}{0.5}\\
\cline{3-4}
&&23.8&21.1&\\
\cline{2-5}
&\multirow{2}{*}{10}&31.1&16.5&\multirow{2}{*}{0.663}\\
\cline{3-4}
&&17.2&35.2&\\
\cline{2-5}
&\multirow{2}{*}{100}&34.9&17.3&\multirow{2}{*}{0.661}\\
\cline{3-4}
&&16.6&31.2&\\
%metric 2
\hline
\multirow{6}{*}{$\mathcal{L}_2$}&\multirow{2}{*}{1}&27.9&24.7&\multirow{2}{*}{0.504}\\
\cline{3-4}
&&24.9&22.5&\\
\cline{2-5}
&\multirow{2}{*}{10}&35.8&13.9&\multirow{2}{*}{0.689}\\
\cline{3-4}
&&17.2&33.1&\\
\cline{2-5}
&\multirow{2}{*}{100}&35&14.1&\multirow{2}{*}{0.707}\\
\cline{3-4}
&&15.2&35.7&\\
%metric inf
\hline
\multirow{6}{*}{$\mathcal{L}_{\infty}$}&\multirow{2}{*}{1}&24.4&26.9&\multirow{2}{*}{0.502}\\
\cline{3-4}
&&22.9&25.8&\\
\cline{2-5}
&\multirow{2}{*}{10}&33.4&21.2&\multirow{2}{*}{0.632}\\
\cline{3-4}
&&15.6&29.8&\\
\cline{2-5}
&\multirow{2}{*}{100}&26.8&24.4&\multirow{2}{*}{0.568}\\
\cline{3-4}
&&18.8&30&\\
\hline
\end{tabular}
\caption{Confusion Matrices for Figure \ref{fig:exp1-2-accuracy-n}}\label{fig:cm-exp1-2-accuracy-n}
\end{figure}
Our experiments showed a positive relationship between accuracy and dimension for all norms up to dimension 10. Norms $\mathcal{L}_1$ and $\mathcal{L}_2$ performed similarly in higher dimensions, with $\mathcal{L}_2$ providing the best accuracy results. Interestingly, for dimensions higher than 20 the accuracy of the $\mathcal{L}_{\infty}$ norm \emph{decreases} as $N$ increases.
\subsubsection{The Effect of Sample Size on Accuracy}
\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{0.75\textwidth}
        \includegraphics[width=\textwidth]{l-experiment1-2-accuracy-k-n-10-s-005-l1.png}
        \caption{$\mathcal{L}_1$, $N=10$}
        \label{fig:exp2k1-1}
    \end{subfigure}
   \begin{subfigure}[h]{0.75\textwidth}
        \includegraphics[width=\textwidth]{l-experiment1-2-accuracy-k-n-100-s-005-l1.png}
        \caption{$\mathcal{L}_1$, $N=100$}
        \label{fig:exp2k2-1}
    \end{subfigure}
\end{figure}
\begin{figure}[H]
\centering
   \ContinuedFloat 
    \begin{subfigure}[h]{0.75\textwidth}
        \includegraphics[width=\textwidth]{l-experiment1-2-accuracy-k-n-10-s-005-l2.png}
        \caption{$\mathcal{L}_2$, $N=10$}
        \label{fig:exp2k1-2}
    \end{subfigure}
   \begin{subfigure}[h]{0.75\textwidth}
        \includegraphics[width=\textwidth]{l-experiment1-2-accuracy-k-n-100-s-005-l2.png}
        \caption{$\mathcal{L}_2$, $N=100$}
        \label{fig:exp2k2-2}
    \end{subfigure}
\end{figure}
\begin{figure}[H]
\centering
   \ContinuedFloat 
    \begin{subfigure}[h]{0.75\textwidth}
        \includegraphics[width=\textwidth]{l-experiment1-2-accuracy-k-n-10-s-005-linf.png}
        \caption{$\mathcal{L}_{\infty}$, $N=10$}
        \label{fig:exp2k1-inf}
    \end{subfigure}
   \begin{subfigure}[h]{0.75\textwidth}
        \includegraphics[width=\textwidth]{l-experiment1-2-accuracy-k-n-100-s-005-linf.png}
        \caption{$\mathcal{L}_{\infty}$, $N=100$}
        \label{fig:exp2k2-inf}
    \end{subfigure}
    \caption{Accuracy as $K$ Varies, $N$ and $\mathcal{L}$ as labeled, $\sigma=.05$}
    \label{fig:exp1-2-accuracy-k}
\end{figure}
\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
%metric 1
$\mathcal{L}$&$K$&\multicolumn{2}{|c|}{Confusion Matrix}&Accuracy\\
\hline
\multirow{6}{*}{$\mathcal{L}_1$}&\multirow{2}{*}{10}&42.9&11.6&\multirow{2}{*}{0.765}\\
\cline{3-4}
&&11.9&33.6&\\
\cline{2-5}
&\multirow{2}{*}{100}&33.6&13.6&\multirow{2}{*}{0.722}\\
\cline{3-4}
&&14.2&38.6&\\
\cline{2-5}
&\multirow{2}{*}{300}&34.9&17.3&\multirow{2}{*}{0.661}\\
\cline{3-4}
&&16.6&31.2&\\
%metric 2
\hline
\multirow{6}{*}{$\mathcal{L}_2$}&\multirow{2}{*}{10}&47.3&12.9&\multirow{2}{*}{0.777}\\
\cline{3-4}
&&9.4&30.4&\\
\cline{2-5}
&\multirow{2}{*}{100}&32.8&15.6&\multirow{2}{*}{0.716}\\
\cline{3-4}
&&12.8&38.8&\\
\cline{2-5}
&\multirow{2}{*}{300}&35&14.1&\multirow{2}{*}{0.707}\\
\cline{3-4}
&&15.2&35.7&\\
%metric inf
\hline
\multirow{6}{*}{$\mathcal{L}_{\infty}$}&\multirow{2}{*}{10}&36.6&13.4&\multirow{2}{*}{0.704}\\
\cline{3-4}
&&16.2&33.8&\\
\cline{2-5}
&\multirow{2}{*}{100}&30.2&18.5&\multirow{2}{*}{0.601}\\
\cline{3-4}
&&21.4&29.9&\\
\cline{2-5}
&\multirow{2}{*}{300}&26.8&24.4&\multirow{2}{*}{0.568}\\
\cline{3-4}
&&18.8&30&\\
\hline
\end{tabular}
\caption{Confusion Matrices for  \Cref{fig:exp2k2-1,fig:exp2k2-2,fig:exp2k2-inf}}\label{fig:cm-exp1-2-accuracy-k}
\end{figure}
Our experiments showed a negative relationship between accuracy and sample size, which weakened as $N$ increased. All three norms exhibited the same behavior, with $\mathcal{L}_2$ performing the best and $\mathcal{L}_{\infty}$ the worst.
\subsubsection{The Effect of Perturbation Variance on Accuracy}
\begin{figure}[H]
    \centering
    \begin{subfigure}[h]{.75\textwidth}
    	\includegraphics[width=\textwidth]{experiment1-2-varsigma-1.png}
   	\caption{$\mathcal{L}_1$}
    \end{subfigure}
    \begin{subfigure}[h]{.75\textwidth}
        \includegraphics[width=\textwidth]{experiment1-2-varsigma-2.png}
   	\caption{$\mathcal{L}_2$}
    \end{subfigure}
\end{figure}
\begin{figure}[H]
\centering
   \ContinuedFloat 
    \begin{subfigure}[h]{.75\textwidth}
        \includegraphics[width=\textwidth]{experiment1-2-varsigma-inf.png}
   	\caption{$\mathcal{L}_{\infty}$}
    \end{subfigure}    
    \caption{Accuracy as $\sigma$ Varies, $\mathcal{L}$ as labeled, $N=100$, $K=300$}
    \label{fig:exp1-2-accuracy-s}
\end{figure}
Our initial range of $\sigma$  did not reveal any significant correlation with accuracy, so the range of $\sigma$ was expanded from 0  to .7 inclusively in .1 increments, keeping $N$ and $K$ constant at 100 and 5000 respectively. 15 trials were run for each $\sigma$ value, and the results for each were averaged. In this expanded range, the accuracy decreased for norms $\mathcal{L}_{2}$ and $\mathcal{L}_{\infty}$, but \emph{increased} for $\mathcal{L}_{1}$.
\subsubsection{On the Number of Classes and Accuracy}
In order to discern the effect of the number of classes on accuracy, we performed additional experiments using class sets of varying cardinality:
\begin{figure}[H]
    \centering
        \includegraphics[width=\textwidth]{exp1-2-varying_classes.png}
    \caption{Accuracy as Class Sets are Varied}\label{fig:exp1-2-varclass}
\end{figure}
In the left half of Figure \ref{fig:exp1-2-varclass}, we see that the positive correlation between accuracy and dimension is preserved from the binary classification experiments, albeit with lower overall accuracy. This trend is demonstrated in the left half of the figure. The data suggest that the relatively high accuracy in the original binary classification is due to the paucity of classification choices.
\section{Conclusion}
\paragraph{}
In conclusion, we have demonstrated that the geometry of high dimensional space greatly impacts the performance of machine learning techniques like nearest neighbor, that rely upon spatial proximity for their results. The distance between points becomes a poor discriminant in these spaces. We have seen that both the distribution of points as well as the metric used must be considered. We have seen that classification accuracy is effected by the dimension of the space as well as the number of classes used in assignment.
\paragraph{}
Future experiments may investigate the performance of additional metrics, such as fractional distance metrics, to see if the trend continues and the performance of the $\mathcal{L}_k$ norm on a uniformly distributed data set increases as k decreases. In addition to normal and uniform distributions, the performance of the distance metrics can be tested on data sets consisting of varying distributions such as multi-modal or correlated data.
\bibliographystyle{plain}
\bibliography{ml-midterm}

\end{document}  